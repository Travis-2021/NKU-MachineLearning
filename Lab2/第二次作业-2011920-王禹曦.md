# MLP_BP实验报告

【网络结构】

- 输入层：

  ​	输入向量：$\pmb x$，维度为S

- 系数矩阵$W'$：$S×M$

- 隐藏层：

  ​	激活函数：`tanh`函数

- 系数矩阵$W'$：$M×N$

- 输出层：

  ​	输出向量：$\pmb y$，维度为M

### 一、损失函数求导

【约定】

- $n_c$表示类别为c的样本数
- $N$表示样本总数

损失函数：
$$
E = \sum_i\sum_j\frac12(y_{i,j}^M-d_{i,j})^2+\frac12\gamma(tr(S_w)-tr(S_b))
$$
分部求导：

- 对第一项求导：

$$
\frac{\partial[\sum_i\sum_j\frac12(y_{i,j}^M-d_{i,j})^2]}{\partial y_{i,j}} =  y_{i,j} -  d_{i,j}
$$

- 对$tr(S_w)$求导：

  展开$tr(S_w)$：

$$
tr(S_w) = \sum_{c=1}^C\sum_{y_i\in c}\sum_j (y_{i,j}-m_{c,j})^2
$$

​	求导：


$$
\begin{aligned}
\frac{\partial \operatorname{tr}\left(S_{w}\right)}{\partial y_{i j}} &=\left(2-\frac{1}{n_c}\right)\left(y_{i j}-m_{i j}\right)+2 \cdot\left(-\frac{1}{n_{c}}\right) \sum_{k \neq i}^{K}\left(y_{k j}-m_{c j}\right) \\
&=2\left(y_{i j}-m_{i j}\right)+2 \cdot\left(-\frac{1}{n_{c}}\right) \sum_{y_{i \in C}}\left(y_{i j}-m_{c j}\right) \\
&=2\left(y_{i j}-m_{c j}\right)
\end{aligned}
$$

- 对$tr(S_b)$求导：

  展开$tr(S_b)$：
  $$
  tr(S_b) = \sum_{c=1}^C\sum_j(m_{cj}-m_j)^2
  $$

  求导：
$$
\begin{aligned}
\frac{\partial tr\left(S_{b}\right)}{\partial y_{i j}} &=2 n_{c}\left(m_{c j}-m_{j}\right)\left(\frac{1}{n_{c}}-\frac{1}{N}\right)+2 n_{c} \cdot\left(-\frac{1}{N}\right) \sum_{R \neq j}^{k}\left(m_{k j}-m_{j}\right) \\
&=2\left(m_{c j}-m_{j}\right)+2 n_{c} \cdot\left(-\frac{1}{N}\right) \sum_{y_{i} \in C}\left(m_{i j}-m_{j}\right) \\
&=2\left(m_{c j}-m_{j}\right)
\end{aligned}
$$

- 综上，

  损失函数对第i个预测向量的某一项求导：

$$
\frac{\partial E}{\partial y_{ij}} =(y_{i,j} -  d_{i,j}) +\gamma(y_{ij}+m_j-2m_{cj})
$$

​	损失函数对第i个预测值求导：
$$
\frac{\partial E}{\partial \pmb{y_{i}}} =(\pmb{y_{i}} -  \pmb{d_{i}}) +\gamma(\pmb{y_{i}}+\pmb{m}-2\pmb{m_c})
$$


### 二、输出层求导

1. 输出层表达式：

$$
\pmb{y}^M = W^{M\times N}\cdot \pmb{z}^N + \pmb{b}^M
$$

2. 损失函数E对系数矩阵$W$求导：

   对某一项求导：

$$
\frac{\partial E}{\partial w_{ij}} =\frac{\partial E}{\partial y_{j}} · \frac{\partial y_i}{\partial w_{ij}} = z_j
$$

​	可知E对$W$求导：
$$
\frac{\partial E}{\partial W} = \left[\begin{array}{ccc}
\frac{\partial E}{y_{1}} \cdot \frac{\partial y_{1}}{\partial w_{11}} & \cdots & \frac{\partial E}{\partial y_{1}} \cdot \frac{\partial y_{1}}{\partial w_{m n}} \\
\frac{\partial E}{y_{2}} \cdot \frac{\partial y_{2}}{\partial u_{21}} & \ddots & \vdots\\
\vdots & \\
\frac{\partial E}{y_{m}} \cdot \frac{\partial y_{m}}{\partial w_{m 1}} & \frac{\partial E}{y_{m}} \cdot \frac{\partial y_{m}}{\partial w_{m 2}} & \frac{\partial E}{y_{m}} \cdot \frac{\partial y_{m}}{\partial w_{m n}}
\end{array}\right]
=\frac{\partial E}{\partial \pmb{y}}\cdot \pmb{z}^T
$$

3. 损失函数E对偏置项$b$求导：

   对某一项求导：

$$
\frac{\partial E}{\partial b_i} = \frac{\partial E}{\partial y_{i}}\cdot \frac{\partial y_i}{\partial b_{i}} = \frac{\partial E}{\partial y_{i}}
$$

​	可知E对$b$求导：
$$
\frac{\partial E}{\partial \pmb b} = \frac{\partial E}{\partial  \pmb y}\cdot \frac{\partial  \pmb y}{\partial \pmb b} = \frac{\partial E}{\partial  \pmb y}
$$

4. 损失函数E对输入向量$\pmb z$求导：

   对某一项求导：

$$
\frac{\partial E}{\partial z_{j}} =\sum_i^M\frac{\partial E}{\partial y_{i}} · \frac{\partial y_i}{\partial z_{j}} = \sum_i^M\frac{\partial E}{\partial y_{i}}w_{ij}
$$

​	可知E对$\pmb z$求导：
$$
\frac{\partial E}{\partial \pmb z} = W^T\cdot\frac{\partial E}{\partial \pmb{y}}
$$

### 三、隐藏层求导

​	本题未说明激活函数，但其对网络的非线性十分重要，所以我选择使用`tanh`函数作为隐藏层的激活函数

设隐藏层输入向量为$\pmb t$

即：
$$
\pmb{z} = tanh(\pmb t)
$$
隐藏层求导：
$$
\frac{\partial \pmb z}{\partial \pmb t} = 1-\pmb z^2
$$
注：其中$\pmb z^2$表示$\pmb z$对位平方运算得到的向量。

### 四、输入层求导

​	设输入向量为$\pmb x$，维度为S，则表达式、系数矩阵求导、偏置项求导均类似输出层。

表达式：
$$
\pmb{t}^N = W'^{N\times S}\cdot \pmb{x}^S + \pmb{b'}^N
$$
系数矩阵$W'$求导：
$$
\frac{\partial E}{\partial W'} =\frac{\partial E}{\partial \pmb z} ⊙\frac{\partial \pmb z}{\partial \pmb t}\cdot \pmb{x}^T
$$
注：其中$⊙$表示向量对位相乘

偏置项$\pmb b'$求导：
$$
\frac{\partial E}{\partial b'} =\frac{\partial E}{\partial \pmb z} ⊙\frac{\partial \pmb z}{\partial \pmb t}
$$

### 五、最终结果计算

将中间计算结果逐层带入，得到最终结果：

- 隐藏层 -> 输出层：


$$
\frac{\partial E}{\partial W}=[(\pmb{y_{i}} -  \pmb{d_{i}}) +\gamma(\pmb{y_{i}}+\pmb{m}-2\pmb{m_c})]\cdot \pmb{z}^T
$$

$$
\frac{\partial E}{\partial \pmb b}  =(\pmb{y_{i}} -  \pmb{d_{i}}) +\gamma(\pmb{y_{i}}+\pmb{m}-2\pmb{m_c})
$$

- 输入层 -> 隐藏层：

$$
\frac{\partial E}{\partial W'} =W^T\cdot[(\pmb{y_{i}} -  \pmb{d_{i}}) +\gamma(\pmb{y_{i}}+\pmb{m}-2\pmb{m_c})] ⊙(1-\pmb z^2)\cdot \pmb{x}^T
$$

$$
\frac{\partial E}{\partial b'} =W^T\cdot[(\pmb{y_{i}} -  \pmb{d_{i}}) +\gamma(\pmb{y_{i}}+\pmb{m}-2\pmb{m_c})]⊙(1-\pmb z^2)
$$

求出导数后，每次进行梯度下降即可。

### 六、总结

​	通过本次实验，我对多层感知机有了更深刻的理解，

​	通过手算梯度对反向传递算法有了更深刻的认识。







